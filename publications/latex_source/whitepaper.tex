\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage{url}
\usepackage{hyperref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{ATSC - A novel approach to monitoring time series compression\\}

\author{\IEEEauthorblockN{1\textsuperscript{st} Carlos Rolo}
\IEEEauthorblockA{\textit{Instaclustr (of Aff.)} \\
\textit{Netapp (of Aff.)}\\
Lisbon, Portugal \\
carlos.rolo@netapp.com}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Joshua Varghese}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{Open SI(NetApp)}\\
Canberra, Australia \\
u3227463@uni.canberra.edu.au}
}

\maketitle

\begin{abstract}
    In this research, we present ATSC (Advanced Time Series Compressor), a ground-breaking approach to monitoring time-series compression. Time series data, representing observations of a single entity over varying time intervals, is a critical component in diverse domains such as climate monitoring and system performance analysis. Our methodology diverges from conventional compression strategies that focus on sample properties or small sequence segments. ATSC treats each time series as a digital signal and incorporates proven techniques from audio and signal compression, emphasizing a unique perspective on lossless compression. Notably, we leverage Function Approximation (FA), commonly used for lossy compression in time series, to address the compression challenge from a lossless standpoint.
    Our research addresses the high-frequency monitoring of computer systems, where signals often lack periodicity or harmonic components. By treating signals holistically and applying audio compression techniques, we achieve not only compression benefits but also exploit the streaming functionality inherent in audio formats. This methodology extends beyond at-rest compression, encompassing in-transit scenarios, thereby reducing egress costs in cloud environments.
    Motivated by the escalating costs of storage and data transfer, as well as the need for efficient computation in processing vast datasets, ATSC introduces innovative techniques in both the write and read paths. The write path involves signal pre-processing, unique packing methodologies, and novel signal identification, leading to a compressed file format. The read path stands out by employing a unique indexing system, enabling precise streaming to relevant sections of the file without storing timestamp information, a feature particularly suited for time-series data.
    Preliminary testing against a Prometheus instance demonstrates significant space savings in the ATSC data directory compared to traditional storage methods. Future work involves exploring concepts such as mutual information and Kullback–Leibler divergence to further enhance compression efficiency.
    In conclusion, ATSC emerges as a promising solution, offering substantial compression gains, efficient storage, and improved data accessibility for time series monitoring.
\end{abstract}

\begin{IEEEkeywords}
Timeseries Compression, Function Approximation, Audio Compression, Data Storage, Streaming, Indexing, Computer Systems Monitoring
\end{IEEEkeywords}

\section{Introduction}
The realm of computer systems monitoring, distinct from general time-series monitoring, is characterized by a unique set of challenges. Typically marked by very low-frequency sampling, ranging from 0.05Hz to 1~2Hz, this process focuses on capturing signals from various processes, such as database operations and the overall health of the operating system [10]. 
 
An intriguing facet of this domain is the absence of periodicity or harmonic components in most signals. In many cases, these signals are treated as isolated samples or short sequences, employing compression techniques like XOR compression and delta-delta encoding. 
However, a paradigm shift emerges by viewing these signals collectively. While the techniques themselves may not be novel, their application in this domain is ground-breaking. By harnessing audio packing and processing methodologies, we not only capitalize on compression benefits but also tap into the extensive streaming functionality embedded in audio formats. 
 
The significance of data compression extends beyond storage concerns to encompass in-transit scenarios, especially with the rise of cloud computing. Sending monitoring data to external systems incurs egress costs, and stored data must be uncompressed and processed before transmission. Leveraging audio packing techniques allows us to seamlessly integrate with the broader ecosystem of software and hardware, ranging from servers to mobile devices, proficient in transporting and encoding/decoding such information on demand. 

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{Fig1.png}
  \caption{Your caption here.}
  \label{Fig.1}
\end{figure}

Our approach doesn't only yield savings in at-rest scenarios but also in transit and during compute processing. By having the ability to leverage client-side devices [9], we enable efficient processing and transport of monitoring data. Integration in system-on-a-Chip architectures [8] could lead to further performance enhancements. This strategy positions our methodology at the forefront of efficient and resource-conscious computer systems monitoring. 

\section{Background}

In the dynamic landscape of computer systems monitoring, the impetus for our research stems from critical challenges that impact both the efficiency and cost-effectiveness of current practices. 

\subsection{High Storage Cost}

The exponential growth of data in computer systems monitoring has led to soaring storage costs. Traditional methods often struggle to cope with the sheer volume of information generated by processes, databases, and overall system health monitoring. Our research delves into innovative time-series compression techniques to alleviate the burden of high storage costs, offering a sustainable solution for data retention. 

\subsection{High Egress Cost}
Sending monitoring data to external systems, especially in cloud environments, incurs in egress costs. Our research recognises the financial implications of these expenses and aims to mitigate them through advanced compression methodologies. By optimising data before transmission, we not only reduce egress costs but also enhance the overall efficiency of data transfer. 

\subsection{Balancing Data Reduction and Information Preservation}
To manage overwhelming data volumes, conventional practices often resort to techniques like averaging, sub-sampling or by not collecting enough information via very low sampling. While effective in reducing data points, this approach comes at a cost – a loss of valuable information. Our research seeks to address this trade-off by proposing advanced time-series compression methods that achieve data reduction without sacrificing critical insights and information. 

\subsection{Computational Challenges in Traditional Monitoring Techniques}
Traditional approaches, such as point-walking algorithms, demand substantial computational resources to process the vast amounts of monitoring data. Recognising the strain on computing capabilities, our research introduces a more resource-efficient paradigm. By exploring novel compression techniques inspired by audio processing, we aim to streamline the computational demands of data analysis in computer systems monitoring. 

  
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{Fig2.png}
  \caption{Your caption here.}
  \label{Fig.2}
\end{figure}

Our comprehensive approach not only tackles the immediate challenges of high storage and egress costs but also addresses the inherent trade-offs in data reduction techniques. The overarching goal is to improve the landscape of computer systems monitoring by introducing a cost-effective and information-rich paradigm through advanced time-series compression. 



\section{Methodology}

\subsection{ATSC Write Paths}

\subsubsection{Pre-processing the Signal for Initial Reduction}
Ahead of compression, the signal undergoes a statistical pre-processing, calculating means, variance, etc. This initial step aims to eliminate redundant information and prepare the signal for more advanced compression methodologies.

\vspace{10pt}
\subsubsection{Packing the Signal in a Non-compressed Format (WAVBRO)}
The uncompressed signal may then be packed into a non-compressed format. For this we created a new format heavily inspired by WAV. The signal can also be stored in the WAV format, but for this frequency might need to be converted, since WAV doesn't support >1 HZ frequencies. Additionally, the signal might be strategically split into integer and float types formatted to sizes compatible with audio sampling. Also, the signal might be split into different channels to reduce size. 

\vspace{10pt}
\subsubsection{Identifying and Exploiting Local Opportunities within the signal}
ATSC leverages its distinctive approach by identifying local opportunities that are more suitable for a mathematical approximation. By slicing the signal in a targeted way, this sets the stage for enhanced compression in the subsequent stages. One way of doing such slicing is via Wavelets transforms, so that high and low frequencie components are identified and the signal divided in different parts accordingly to its behaviour.

\vspace{10pt}
\subsubsection{Generate an Index for Precision Access}
To facilitate precise access to samples without the need for full file seeking or decompression of the full file, ATSC generates a comprehensive index. This index ensures efficient retrieval of relevant portions of the signal, a critical aspect in the subsequent compression and decompression processes.

\vspace{10pt}
\subsubsection{Blocking and Modelling}
The signal samples are divided into blocks, and for each block, ATSC applies modelling techniques tailored to the specific characteristics of the signal. This may involve techniques such as Linear Predictive Coding (LPC), Polynomial Prediction, Fast Fourier Transforms (FFT), or other methodologies that best suit the signal's behaviour. This modeling converts the original samples into an approximated formula. In some cases it is possible to match the original signal precisely. This leads to a high number of data points being converted into a mathematical formula and its components.

\vspace{10pt}
\subsubsection{Error correction and final optimizations}
In cases where the resulting formulas are approximated, error correction can be applied. A error percentage can be provided so that the amount of error correction is applied until the approximation + error correction falls within the provided error margin. After the error corrections are applied (either via keeping samples with the highest error, Huffman-codding differences, etc) further optimization is done by reducing the information to the minimal computational size needed for each sample. Samples are normally provided in 64bits size, this step can reduce the sample size to 8bit if the samples fit.

\vspace{10pt}
\subsubsection{Generate the Final Compressed File}
Building upon the insights gained from steps 1 to 7, ATSC culminates the write path by generating the final compressed file. This file encapsulates the intricacies of the original signal in a highly compressed and efficient format, ready for storage or transmission.


\subsection{ATSC Read Paths}

Within the Read Path of ATSC, a ground-breaking methodology unfolds, emphasizing the unique utilization of indexing for precise data retrieval. Notably, ATSC distinguishes itself by eschewing the storage of timestamp information directly within the file, opting instead for a distinctive indexing approach that enables efficient streaming and targeted decompression of pertinent data segments.

\vspace{10pt}
\subsubsection{Precise Streaming through Specialized Indexing}\label{SCM}
The core innovation in the Read Path lies in the application of a specialized indexing mechanism. Unlike traditional methods, ATSC's indexing facilitates pinpointing the relevant portion of the file without the necessity of timestamp inclusion in the file itself. This precision in indexing empowers ATSC to streamline the streaming process, focusing exclusively on the required data, thereby optimizing resource utilization. 

\vspace{10pt}
\subsubsection{Breaking Ground in Monitoring Data Retrieval}\label{SCM}
While the standard practice of streaming by blocks and subsequent decompression is commonplace in multimedia file handling, ATSC introduces a paradigm shift when applied to the distinct requirements of monitoring data. This unique approach sets ATSC apart, presenting an innovative means of handling time series data that has not been extensively documented or explored in the monitoring space. 
\vspace{10pt}
\subsubsection{Unveiling of the Read Path}\label{SCM}
\begin{itemize}

\item{\textbf{\textit{Identifying the Metric to be Queried:}}} The Read Path initiation begins with the user specifying the metric of interest. This action triggers the subsequent retrieval process.
\vspace{5pt}

\item{\textbf{\textit{Locate Files and Corresponding Index:}}} ATSC efficiently locates pertinent file(s) and their corresponding indices. Leveraging the indexing system ensures accuracy in file identification.
\vspace{5pt}
\item{\textbf{\textit{Precision in Data Retrieval Using the Index:}}} The index plays a crucial role in ATSC's approach. It aids in precisely identifying the segment of the file essential for the query. This ensures focused and efficient data retrieval.
\vspace{5pt}
\item{\textbf{\textit{Streaming Blocks to the Client:}}} Once the relevant data segment is identified, ATSC initiates streaming of these blocks. Direct streaming optimizes data transfer, particularly beneficial in scenarios with limited bandwidth or transmission constraints.
\vspace{5pt}
\item{\textbf{\textit{Decompression and Extraction by the Client:}}} The client, equipped with the streamed data blocks, handles decompression and extraction. This decentralized approach enhances efficiency by distributing the computational load.
\vspace{5pt}
\item{\textbf{\textit{Timestamp Integration Post-Decompression:}}} Following decompression, the samples, now in their extracted form, receive timestamp information from the index. This final step ensures temporal accuracy and relevance of the retrieved data.
\end{itemize}

\subsection{Indexing Samples with VRSI (Variable Rate Sampling Interval)}

\subsubsection{Overview of VRSI Mechanism}

In the methodology section, a critical aspect of the process involves indexing samples for efficient data retrieval. Variable Rate Sampling Interval (VRSI) is employed to manage the sampling intervals effectively. VRSI operates on the premise that samples occur at evenly spaced intervals, such as every 5 seconds, 20 seconds, or 60 seconds.

\subsubsection{Line Segment Representation}

For each sampling interval, a line segment is created with specific fields:

\begin{itemize}
    \item \textbf{Start Timestamp:} The timestamp marking the beginning of the sampling interval.
    \item \textbf{Sample Interval:} The time gap between consecutive samples.
    \item \textbf{Starting Sample:} The index of the first sample within the segment.
    \item \textbf{Number of Samples:} The total number of samples within the segment.
\end{itemize}

\subsubsection{File Structure}

These line segments are then stored in a file, accompanied by the lowest and highest timestamps in the segments represented. This file structure allows for easy identification of whether a requested interval is present in the index. If the requested interval falls entirely outside the timestamps in the file header, no samples are available for that interval.

\subsubsection{Handling Temporal Gaps}

In cases where a metric stops reporting for a period, a new line segment is generated, ensuring accurate representation of the sampled data over time.

\subsubsection{Example VRSI File Content}

\begin{enumerate}
    \item 55745
    \item 59435
    \item 15,0,55745,166
    \item 15,166,58505,63
\end{enumerate}


\begin{itemize}
    \item Line 1) Represents the first timestamp.
    \item Line 2) Represents the last timestamp.
    \item Line segments are detailed below:
    \begin{itemize}
        \item The first line segment has one sample every 15 seconds, starting at timestamp 55745, with a total of 166 samples.
        \item The second line segment also has one sample every 15 seconds, starting at timestamp 58505, with 63 samples.
    \end{itemize}
\end{itemize}

\subsubsection{Locating Samples (Read Path)}

In the read path, when locating a sample in a file using the index, timestamps or timestamp ranges are specified (e.g., "All the samples from 3:30 PM to 4:30 PM"). The system checks if the requested timestamps are within the available timestamps in the file header. If found, the sequence of sample numbers is extracted, indicating the samples needed for decompression.

\subsubsection{Creating/Updating the Index (Write Path)}

When a new sample is added, the index is updated. Since time progresses linearly, and samples occur in sequence, the system only needs to:
\begin{itemize}
    \item Update the last segment's sample number or
    \item Create a new segment.
\end{itemize}

Existing segments are incremented if the sample timestamp is the next in sequence. If a segment does not exist or the timestamp is not the next in sequence for the latest segment, a new segment is created. This approach ensures an efficient and organized index for managing variable rate sampling intervals.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{Fig3.png}
  \caption{Your caption here.}
  \label{Fig.3}
\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{Fig4.png}
  \caption{Your caption here.}
  \label{Fig.4}
\end{figure}




\section{Results}

In our quest to assess the efficiency and effectiveness of BRRO, a comprehensive testing methodology was employed. The ATSC server was configured as both a read and write backend for a Prometheus instance, establishing a practical and relevant testing environment. This Prometheus instance, in turn, was connected to an Instaclustr 3-node Cassandra cluster with a Prometheus endpoint enabled. The testing duration, as indicated in the results, spanned a defined period.

\subsection*{ATSC Single: A Best-Case Scenario Analysis}

Before delving into the detailed results, it is essential to address the ATSC single scenario. This represents an optimal output expected on a server running for an extended period, devoid of headers and enriched with substantial data conducive to efficient compression. Although this scenario does not reflect a realistic measure, it serves as a best-case scenario for the current test. The approach involves consolidating all data from every file into a single file, subsequently compressed.

\subsection*{Data Overview}

\textbf{Raw Data Size:} 261,370,032 bytes \\
\textbf{Compression Statistics:}

\begin{itemize}
    \item \textbf{ATSC Compression:}
    \begin{itemize}
        \item Compressed Size: 4,558,363 bytes
        \item Compression Ratio: 57.34 times
    \end{itemize}
    \item \textbf{LZ4 Compression:}
    \begin{itemize}
        \item Compressed Size: 30,634,699 bytes
        \item Compression Ratio: 8.53 times
    \end{itemize}
    \item \textbf{FLAC Compression:}
    \begin{itemize}
        \item Compressed Size: 46,524,174 bytes
        \item Compression Ratio: 5.62 times
    \end{itemize}
\end{itemize}

\textbf{Signal Count:}

\begin{itemize}
    \item Total Signals: 5,860
\end{itemize}

\textbf{Compression Ratios Analysis:}

\begin{itemize}
    \item Median Compression Ratio: 1,653.82
    \item Average Compression Ratio: 1,979.21
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{Fig5.png}
  \caption{Your caption here.}
  \label{Fig.5}
\end{figure}

\vspace{10pt}
These results offer a detailed insight into the performance of ATSC across various compression techniques. The data overview provides the baseline, while compression statistics and signal count shed light on the efficiency of ATSC's compression strategies. The analysis of compression ratios, both median and average, further quantifies the compression effectiveness across the tested scenarios. 
 

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{cpu.png}
  \caption{Your caption here.}
  \label{cpu}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Disk.png}
    \caption{Your caption here.}
    \label{Disk}
  \end{figure}

  \begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{heap.png}
    \caption{Your caption here.}
    \label{heap}
  \end{figure}
\vspace{10pt}
The ATSC Compression exhibited an impressive compression ratio of 57.34 times, showcasing the potential of BRRO in achieving substantial data reduction. Additionally, the LZ4 and FLAC Compression techniques demonstrated notable compression ratios of 8.53 and 5.62 times, respectively, highlighting the versatility of ATSC in adapting to different compression scenarios. 
These results substantiate ATSC's capabilities in optimizing storage space, offering valuable insights into its potential impact on real-world applications. 

\section{FUTURE WORK}

In the pursuit of continual improvement and innovation, the following avenues represent promising areas for future exploration within the context of ATSC.

\subsection{Automated Selections: Embracing AI Advancements}

Enhancing User Experience: Investigate the potential benefits of incorporating automated selections, possibly powered by Artificial Intelligence (AI). This exploration aims to streamline user interactions and optimize compression choices intelligently.

\subsection*{Integration with Time Series Databases/Services Seamless Integration}

Extend ATSC's functionality by integrating seamlessly with Time Series databases and services. This integration holds the promise of providing users with a cohesive and efficient solution for managing Time Series data.

\subsection{Enhanced Functionality}

Explore avenues to enhance ATSC's functionality within the realm of Time Series databases, ensuring that users experience a unified and effective solution for their data management needs.

\subsection{Streaming: Leveraging Insights from FLAC Experiments}

Building on Experience: Draw insights from early experiments, particularly those involving Free Lossless Audio Codec (FLAC). Utilize the lessons learned to inform and refine ATSC's performance in streaming scenarios.

\subsection{Optimizing Streaming Capabilities}

Apply the knowledge gained from FLAC experiments to optimize ATSC's streaming capabilities. This involves fine-tuning the compression strategy to excel in scenarios where real-time or efficient streaming of Time Series data is paramount.

These future directions underscore ATSC's commitment to continuous improvement and adaptation to emerging technologies. By exploring automated selections, integrating with Time Series databases, and leveraging lessons from streaming experiences, ATSC aims to stay at the forefront of efficient and intelligent Time Series data compression. This forward-looking section sets the stage for ongoing advancements and ensures that ATSC remains a versatile and cutting-edge solution in the dynamic landscape of data compression.


\section{CONCLUSION}

In conclusion, the ATSC approach outlined in this research offers a novel solution for monitoring high-frequency time series data, particularly in computer systems. By applying proven audio compression techniques, ATSC achieves impressive compression ratios, with a best-case scenario reaching 57.34 times. This innovative methodology not only addresses current storage and data transfer challenges but also sets the stage for future improvements.
\vspace{5pt}
Preliminary testing against a Prometheus instance demonstrates significant space savings in the ATSC data directory compared to traditional methods. The research emphasizes the importance of unique indexing for precise data retrieval, showcasing its efficiency in streaming and targeted decompression of relevant data segments.

The outlined future work underscores ATSC's commitment to continuous improvement, with a focus on automated selections, integration with time series databases, and optimizing streaming capabilities. These efforts aim to keep ATSC at the forefront of efficient and intelligent time series data compression.

In summary, ATSC emerges as a promising solution that not only tackles current challenges but also paves the way for ongoing advancements in the dynamic landscape of high-frequency time series monitoring and data compression.

\section{References}

\begin{enumerate}
    \item InfluxData. (n.d.). What is Time Series Data? [Online]. Available: \url{https://www.influxdata.com/what-is-time-series-data/} [1]

    \item Teller, J., et al. (2015). Gorilla: A Fast, Scalable, In-Memory Time Series Database. Proceedings of the VLDB Endowment, 8(12), 1816-1827. PDF [2]

    \item VictoriaMetrics. (n.d.). Achieving Better Compression for Time Series Data than Gorilla. [Online]. Available: \href{https://faun.pub/victoriametrics-achieving-better-compression-for-time-series-data-than-gorilla-317bc1f95932}{https://faun.pub/victoriametrics-achieving-better-compression-for-time-series-data-than-gorilla-317bc1f95932} [3]

    \item TimeScale. (n.d.). Time Series Compression Algorithms Explained. [Online]. Available: \url{https://www.timescale.com/blog/time-series-compression-algorithms-explained/} [4]

    \item DZone. (n.d.). Time Series Compression Algorithms and Their Applications. [Online]. Available: \url{https://dzone.com/articles/time-series-compression-algorithms-and-their-appli#:~:text=Time%20series%20compression%20algorithms%20take%20advantage%20of%20specific,functions%20or%20predicting%20them%20through%20neural%20network%20models} [5]

    \item Yang, S., et al. (2021). An Intelligent Compression Algorithm for Time Series Data. PDF [6]

    \item Gunasekaran, M., et al. (2019). SmartFusion CSOC Implementation of FLAC Player Using Hardware and Software Partitioning. [Online]. Available: \href{https://www.microsemi.com/document-portal/doc_view/129825-ac376-smartfusion-csoc-implementation-of-flac-player-using-hardware-and-software-partitioning-app-note} [7]

    \item Wikipedia. (n.d.). List of Hardware and Software that Supports FLAC. [Online]. Available: \url{https://en.wikipedia.org/wiki/List_of_hardware_and_software_that_supports_FLAC} [8]

    \item Apache Cassandra. (n.d.). Operating Metrics. [Online]. Available: \url{https://cassandra.apache.org/doc/latest/cassandra/operating/metrics.html} [9]

    \item Free Patents Online. (n.d.). Data Compression Method and Apparatus. [Online]. Available: \url{https://www.freepatentsonline.com/5839100.html} [10]
\end{enumerate}

\end{document}